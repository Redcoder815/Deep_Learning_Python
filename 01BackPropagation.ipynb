{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_Python/blob/main/01BackPropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.weights_input_hidden = np.random.randn(\n",
        "            self.input_size, self.hidden_size)\n",
        "        self.weights_hidden_output = np.random.randn(\n",
        "            self.hidden_size, self.output_size)\n",
        "\n",
        "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
        "        self.bias_output = np.zeros((1, self.output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def feedforward(self, X):\n",
        "        self.hidden_activation = np.dot(\n",
        "        X, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_output = self.sigmoid(self.hidden_activation)\n",
        "\n",
        "        self.output_activation = np.dot(\n",
        "        self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.predicted_output = self.sigmoid(self.output_activation)\n",
        "\n",
        "        return self.predicted_output\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        output_error = y - self.predicted_output\n",
        "        output_delta = output_error * \\\n",
        "        self.sigmoid_derivative(self.predicted_output)\n",
        "\n",
        "        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
        "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        self.weights_hidden_output += np.dot(self.hidden_output.T,\n",
        "                                         output_delta) * learning_rate\n",
        "        self.bias_output += np.sum(output_delta, axis=0,\n",
        "                               keepdims=True) * learning_rate\n",
        "        self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate\n",
        "        self.bias_hidden += np.sum(hidden_delta, axis=0,\n",
        "                               keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            output = self.feedforward(X)\n",
        "            self.backward(X, y, learning_rate)\n",
        "            if epoch % 4000 == 0:\n",
        "                loss = np.mean(np.square(y - output))\n",
        "                print(f\"Epoch {epoch}, Loss:{loss}\")\n",
        "\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "nn.train(X, y, epochs=10000, learning_rate=0.1)\n",
        "\n",
        "output = nn.feedforward(X)\n",
        "print(\"Predictions after training:\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_pJLEgPtZpy",
        "outputId": "fbfba76d-a13f-4b8c-ae7c-4b77726cb3fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss:0.2639659691915888\n",
            "Epoch 4000, Loss:0.006595200421080553\n",
            "Epoch 8000, Loss:0.0022121611755944754\n",
            "Predictions after training:\n",
            "[[0.02510323]\n",
            " [0.95959851]\n",
            " [0.95893597]\n",
            " [0.05081071]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05a8a7cb"
      },
      "source": [
        "### The Mathematical Basis: The Chain Rule\n",
        "\n",
        "In a neural network, our goal is to minimize a **Loss Function (L)** by adjusting the network's weights and biases. To do this, we need to calculate the *gradient* of the loss with respect to each weight and bias â€“ that is, how much the loss changes when a specific weight or bias changes. This is where derivatives come in.\n",
        "\n",
        "Consider a simple path in your network:\n",
        "\n",
        "1.  **Input `X`** goes into a layer.\n",
        "2.  It's multiplied by **Weights `W`** and added to **Bias `b`** to get an **Activation `Z`** (`Z = XW + b`).\n",
        "3.  `Z` is then passed through an **Activation Function `f`** (like sigmoid) to get the **Output `A`** (`A = f(Z)`).\n",
        "4.  This `A` then contributes to the **Loss `L`**.\n",
        "\n",
        "To find out how a weight `W` affects the Loss `L` (i.e., `dL/dW`), we use the chain rule:\n",
        "\n",
        "`dL/dW = (dL/dA) * (dA/dZ) * (dZ/dW)`\n",
        "\n",
        "Let's look at each term:\n",
        "\n",
        "*   **`dL/dA`**: This is the gradient of the Loss with respect to the output of the activation function. It tells us how sensitive the loss is to changes in the output of this particular layer.\n",
        "\n",
        "*   **`dA/dZ`**: This is the **derivative of the activation function** with respect to its input. If `A = f(Z)`, then `dA/dZ = f'(Z)`. **This is precisely where `self.sigmoid_derivative(self.hidden_activation)` or `self.sigmoid_derivative(self.output_activation)` comes from in your code.**\n",
        "\n",
        "*   **`dZ/dW`**: This is the derivative of the activation `Z` with respect to the weight `W`. Since `Z = XW + b`, then `dZ/dW = X` (the input to the layer).\n",
        "\n",
        "### Connecting to Your `backward` Function\n",
        "\n",
        "Let's map this to the `output_delta` calculation in your code:\n",
        "\n",
        "```python\n",
        "output_error = y - self.predicted_output  # This is proportional to dL/dA_output\n",
        "output_delta = output_error * self.sigmoid_derivative(self.predicted_output)\n",
        "```\n",
        "\n",
        "In this context, `output_error` (or a related error term) essentially captures `dL/dA_output`. Then, `self.sigmoid_derivative(self.predicted_output)` calculates `dA_output / dZ_output`, where `predicted_output` is `A_output` and the argument to the `sigmoid_derivative` should ideally be the pre-activation `Z_output`. However, since `f'(Z) = f(Z)(1-f(Z)) = A(1-A)`, you correctly use `self.predicted_output` (which is `A_output`) directly.\n",
        "\n",
        "So, `output_delta` becomes proportional to `(dL/dA_output) * (dA_output/dZ_output)`. This is the `dL/dZ_output` term for the output layer.\n",
        "\n",
        "Similarly, for the hidden layer:\n",
        "\n",
        "```python\n",
        "hidden_error = np.dot(output_delta, self.weights_hidden_output.T) # This propagates the error dL/dZ_output back to become dL/dA_hidden\n",
        "hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
        "```\n",
        "\n",
        "Here, `hidden_error` represents `dL/dA_hidden` (how much the loss is affected by the hidden layer's output). Then, `self.sigmoid_derivative(self.hidden_output)` is `dA_hidden / dZ_hidden`. Multiplying them gives `dL/dZ_hidden`.\n",
        "\n",
        "### Summary\n",
        "\n",
        "The `backward` function *implements* the chain rule. The `sigmoid_derivative` function provides the `dA/dZ` part of that chain. By multiplying the error propagated from subsequent layers (`dL/dA`) by the derivative of the activation function (`dA/dZ`), we effectively compute `dL/dZ` for a given layer. This `dL/dZ` is then used to find the gradients with respect to weights (`dL/dW = (dL/dZ) * (dZ/dW)`) and biases (`dL/db = (dL/dZ) * (dZ/db)`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a979112"
      },
      "source": [
        "The derivative plays a crucial role in the `backward` function, which implements the backpropagation algorithm for training the neural network. Specifically, the derivative of the activation function (in this case, the sigmoid function) is used to calculate how much the weights and biases should be adjusted.\n",
        "\n",
        "### The Sigmoid Function and its Derivative\n",
        "\n",
        "Your `NeuralNetwork` class uses the sigmoid activation function, defined as:\n",
        "\n",
        "```python\n",
        "def sigmoid(self, x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "```\n",
        "\n",
        "The derivative of the sigmoid function, `f'(x)`, when `x` is the output of the sigmoid function `f(z)`, is `f(z) * (1 - f(z))`. Your implementation reflects this:\n",
        "\n",
        "```python\n",
        "def sigmoid_derivative(self, x):\n",
        "    # Here, 'x' is assumed to be the output of the sigmoid function\n",
        "    return x * (1 - x)\n",
        "```\n",
        "\n",
        "### How the Derivative is Used in the `backward` Function\n",
        "\n",
        "The `backward` function calculates the error at the output layer and then propagates it back through the network to update the weights and biases. The `sigmoid_derivative` is essential for this process:\n",
        "\n",
        "1.  **Output Layer Delta Calculation:**\n",
        "    ```python\n",
        "    output_error = y - self.predicted_output\n",
        "    output_delta = output_error * self.sigmoid_derivative(self.predicted_output)\n",
        "    ```\n",
        "    *   `output_error`: This is the difference between the actual target `y` and the `predicted_output` of the network.\n",
        "    *   `self.sigmoid_derivative(self.predicted_output)`: This calculates the derivative of the sigmoid function with respect to the `predicted_output`. Multiplying the `output_error` by this derivative gives us `output_delta`, which indicates how much the output weights need to change to reduce the error.\n",
        "\n",
        "2.  **Hidden Layer Delta Calculation:**\n",
        "    ```python\n",
        "    hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
        "    hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
        "    ```\n",
        "    *   `hidden_error`: This error is propagated back from the output layer to the hidden layer. It's calculated by taking the dot product of `output_delta` with the transpose of the `weights_hidden_output`.\n",
        "    *   `self.sigmoid_derivative(self.hidden_output)`: Similar to the output layer, this calculates the derivative of the sigmoid function for the `hidden_output`. Multiplying `hidden_error` by this derivative gives `hidden_delta`, which tells us how much the hidden layer weights and biases need to be adjusted.\n",
        "\n",
        "These `delta` values are then used to update the weights and biases using the `learning_rate` to minimize the overall loss of the neural network."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}