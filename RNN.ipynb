{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBIa6MI0NhXwb7rIx6bDHY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_Python/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class RNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Xavier initialization\n",
        "        self.Wxh = np.random.randn(hidden_size, input_size) / np.sqrt(input_size)\n",
        "        self.Whh = np.random.randn(hidden_size, hidden_size) / np.sqrt(hidden_size)\n",
        "        self.Why = np.random.randn(output_size, hidden_size) / np.sqrt(hidden_size)\n",
        "\n",
        "        self.bh = np.zeros((hidden_size, 1))\n",
        "        self.by = np.zeros((output_size, 1))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        e = np.exp(x - np.max(x))\n",
        "        return e / np.sum(e)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        inputs: list of one-hot vectors (input_size x 1)\n",
        "        returns: (outputs, hidden_states)\n",
        "        \"\"\"\n",
        "        h = np.zeros((self.hidden_size, 1))\n",
        "        hs = {-1: h}\n",
        "        ys = {}\n",
        "        ps = {}\n",
        "\n",
        "        for t in range(len(inputs)):\n",
        "            x = inputs[t]\n",
        "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
        "            y = self.Why @ h + self.by\n",
        "            p = self.softmax(y)\n",
        "\n",
        "            hs[t] = h\n",
        "            ys[t] = y\n",
        "            ps[t] = p\n",
        "\n",
        "        return ys, ps, hs\n",
        "\n",
        "    def backward(self, inputs, targets, ps, hs, learning_rate=1e-2):\n",
        "        \"\"\"\n",
        "        Cross-entropy loss + BPTT\n",
        "        targets: list of integer class indices\n",
        "        \"\"\"\n",
        "        dWxh = np.zeros_like(self.Wxh)\n",
        "        dWhh = np.zeros_like(self.Whh)\n",
        "        dWhy = np.zeros_like(self.Why)\n",
        "        dbh = np.zeros_like(self.bh)\n",
        "        dby = np.zeros_like(self.by)\n",
        "\n",
        "        dh_next = np.zeros_like(hs[0])\n",
        "        loss = 0\n",
        "\n",
        "        for t in reversed(range(len(inputs))):\n",
        "            p = ps[t]\n",
        "            target_idx = targets[t]\n",
        "\n",
        "            # Cross-entropy loss\n",
        "            loss += -np.log(p[target_idx, 0])\n",
        "\n",
        "            # Gradient of softmax + CE\n",
        "            dy = p.copy()\n",
        "            dy[target_idx] -= 1  # dL/dy\n",
        "\n",
        "            dWhy += dy @ hs[t].T\n",
        "            dby += dy\n",
        "\n",
        "            dh = self.Why.T @ dy + dh_next\n",
        "            dh_raw = (1 - hs[t] ** 2) * dh\n",
        "\n",
        "            dbh += dh_raw\n",
        "            dWxh += dh_raw @ inputs[t].T\n",
        "            dWhh += dh_raw @ hs[t-1].T\n",
        "\n",
        "            dh_next = self.Whh.T @ dh_raw\n",
        "\n",
        "        # Clip exploding gradients\n",
        "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "            np.clip(dparam, -5, 5, out=dparam)\n",
        "\n",
        "        # Update parameters\n",
        "        self.Wxh -= learning_rate * dWxh\n",
        "        self.Whh -= learning_rate * dWhh\n",
        "        self.Why -= learning_rate * dWhy\n",
        "        self.bh  -= learning_rate * dbh\n",
        "        self.by  -= learning_rate * dby\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "# Example: vocabulary of 5 symbols\n",
        "vocab_size = 5\n",
        "hidden_size = 16\n",
        "rnn = RNN(vocab_size, hidden_size, vocab_size)\n",
        "\n",
        "def one_hot(idx, size):\n",
        "    v = np.zeros((size, 1))\n",
        "    v[idx] = 1\n",
        "    return v\n",
        "\n",
        "for epoch in range(2000):\n",
        "    # Random sequence\n",
        "    seq_len = 4\n",
        "    inputs = [one_hot(np.random.randint(vocab_size), vocab_size) for _ in range(seq_len)]\n",
        "    targets = [np.random.randint(vocab_size) for _ in range(seq_len)]\n",
        "\n",
        "    ys, ps, hs = rnn.forward(inputs)\n",
        "    loss = rnn.backward(inputs, targets, ps, hs, learning_rate=1e-2)\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-fN_3EjH5ch",
        "outputId": "eba5c5d4-4131-4f95-d06f-625d4086d23b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 5.7268\n",
            "Epoch 200, Loss: 5.9601\n",
            "Epoch 400, Loss: 6.5577\n",
            "Epoch 600, Loss: 6.2616\n",
            "Epoch 800, Loss: 6.1602\n",
            "Epoch 1000, Loss: 6.1239\n",
            "Epoch 1200, Loss: 6.4440\n",
            "Epoch 1400, Loss: 6.4738\n",
            "Epoch 1600, Loss: 6.5945\n",
            "Epoch 1800, Loss: 6.1128\n"
          ]
        }
      ]
    }
  ]
}