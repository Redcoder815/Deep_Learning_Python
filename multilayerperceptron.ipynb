{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpdM3suG6XGjfl+vpEI7qv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_Python/blob/main/multilayerperceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function INIT(layer_sizes, activation):\n",
        "#     num_layers = length(layer_sizes) - 1\n",
        "\n",
        "#     for each layer i:\n",
        "#         initialize weight matrix W[i] with random values\n",
        "#         initialize bias vector b[i] with zeros\n",
        "\n",
        "#     choose activation function and its derivative\n",
        "\n",
        "# function FORWARD(X):\n",
        "#     activations = [X]\n",
        "#     preactivations = []\n",
        "\n",
        "#     for each hidden layer i:\n",
        "#         z = activations[i] * W[i] + b[i]\n",
        "#         a = activation(z)\n",
        "#         store z and a\n",
        "\n",
        "#     # Output layer uses softmax\n",
        "#     z_last = last_activation * W_last + b_last\n",
        "#     a_last = softmax(z_last)\n",
        "\n",
        "#     return activations, preactivations\n",
        "\n",
        "# function BACKWARD(activations, preactivations, y_true):\n",
        "#     convert y_true to one-hot encoding\n",
        "\n",
        "#     # Output layer gradient\n",
        "#     delta = (softmax_output - one_hot_labels) / batch_size\n",
        "\n",
        "#     for each layer i from last to first:\n",
        "#         grad_W[i] = activations[i]^T * delta\n",
        "#         grad_B[i] = sum(delta over batch)\n",
        "\n",
        "#         if not first layer:\n",
        "#             delta = (delta * W[i]^T) * activation_derivative(preactivations[i-1])\n",
        "\n",
        "#     return grad_W, grad_B\n",
        "\n",
        "# function UPDATE(grad_W, grad_B, learning_rate):\n",
        "#     for each layer i:\n",
        "#         W[i] = W[i] - learning_rate * grad_W[i]\n",
        "#         B[i] = B[i] - learning_rate * grad_B[i]\n",
        "\n",
        "# function LOSS(X, y):\n",
        "#     compute forward pass to get probabilities\n",
        "#     extract probability of correct class for each sample\n",
        "#     compute mean negative log likelihood\n",
        "#     return loss\n",
        "\n",
        "# function FIT(X, y, epochs, learning_rate, batch_size):\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "\n",
        "#         shuffle dataset\n",
        "\n",
        "#         for each batch:\n",
        "#             X_batch, y_batch = next batch\n",
        "\n",
        "#             activations, preactivations = FORWARD(X_batch)\n",
        "#             grad_W, grad_B = BACKWARD(activations, preactivations, y_batch)\n",
        "#             UPDATE(grad_W, grad_B, learning_rate)\n",
        "\n",
        "#         optionally print loss every N epochs\n",
        "\n",
        "# function PREDICT(X):\n",
        "#     compute forward pass\n",
        "#     return index of max probability for each sample\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Activation functions\n",
        "# -----------------------------\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "# -----------------------------\n",
        "# MLP Class\n",
        "# -----------------------------\n",
        "class MLP:\n",
        "    def __init__(self, layer_sizes, activation=\"relu\", seed=42):\n",
        "        \"\"\"\n",
        "        layer_sizes: list like [input_dim, hidden1, hidden2, ..., output_dim]\n",
        "        activation: \"relu\" or \"sigmoid\"\n",
        "        \"\"\"\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        self.num_layers = len(layer_sizes) - 1\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Initialize weights\n",
        "        for i in range(self.num_layers):\n",
        "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2 / layer_sizes[i])\n",
        "            b = np.zeros((1, layer_sizes[i+1]))\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "\n",
        "        # Activation choice\n",
        "        if activation == \"relu\":\n",
        "            self.activation = relu\n",
        "            self.activation_derivative = relu_derivative\n",
        "        else:\n",
        "            self.activation = sigmoid\n",
        "            self.activation_derivative = sigmoid_derivative\n",
        "\n",
        "    # -----------------------------\n",
        "    # Forward pass\n",
        "    # -----------------------------\n",
        "    def forward(self, X):\n",
        "        a = X\n",
        "        activations = [X]\n",
        "        zs = []\n",
        "\n",
        "        for i in range(self.num_layers - 1):\n",
        "            z = a @ self.weights[i] + self.biases[i]\n",
        "            a = self.activation(z)\n",
        "            zs.append(z)\n",
        "            activations.append(a)\n",
        "\n",
        "        # Output layer (softmax)\n",
        "        z = activations[-1] @ self.weights[-1] + self.biases[-1]\n",
        "        exp_scores = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        a = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        zs.append(z)\n",
        "        activations.append(a)\n",
        "\n",
        "        return activations, zs\n",
        "\n",
        "    # -----------------------------\n",
        "    # Backpropagation\n",
        "    # -----------------------------\n",
        "    def backward(self, activations, zs, y_true):\n",
        "        m = y_true.shape[0]\n",
        "        grads_w = [None] * self.num_layers\n",
        "        grads_b = [None] * self.num_layers\n",
        "\n",
        "        # One-hot encode labels\n",
        "        y_onehot = np.zeros_like(activations[-1])\n",
        "        y_onehot[np.arange(m), y_true] = 1\n",
        "\n",
        "        # Output layer gradient\n",
        "        delta = (activations[-1] - y_onehot) / m\n",
        "\n",
        "        # Backprop through layers\n",
        "        for i in reversed(range(self.num_layers)):\n",
        "            grads_w[i] = activations[i].T @ delta\n",
        "            grads_b[i] = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "            if i > 0:\n",
        "                delta = (delta @ self.weights[i].T) * self.activation_derivative(zs[i-1])\n",
        "\n",
        "        return grads_w, grads_b\n",
        "\n",
        "    # -----------------------------\n",
        "    # Parameter update\n",
        "    # -----------------------------\n",
        "    def update(self, grads_w, grads_b, lr):\n",
        "        for i in range(self.num_layers):\n",
        "            self.weights[i] -= lr * grads_w[i]\n",
        "            self.biases[i] -= lr * grads_b[i]\n",
        "\n",
        "    # -----------------------------\n",
        "    # Training loop\n",
        "    # -----------------------------\n",
        "    def fit(self, X, y, epochs=1000, lr=0.01, batch_size=32):\n",
        "        n = X.shape[0]\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            idx = np.random.permutation(n)\n",
        "            X_shuffled = X[idx]\n",
        "            y_shuffled = y[idx]\n",
        "\n",
        "            for start in range(0, n, batch_size):\n",
        "                end = start + batch_size\n",
        "                X_batch = X_shuffled[start:end]\n",
        "                y_batch = y_shuffled[start:end]\n",
        "\n",
        "                activations, zs = self.forward(X_batch)\n",
        "                grads_w, grads_b = self.backward(activations, zs, y_batch)\n",
        "                self.update(grads_w, grads_b, lr)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                loss = self.loss(X, y)\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Loss function\n",
        "    # -----------------------------\n",
        "    def loss(self, X, y):\n",
        "        activations, _ = self.forward(X)\n",
        "        probs = activations[-1]\n",
        "        m = y.shape[0]\n",
        "        log_likelihood = -np.log(probs[np.arange(m), y] + 1e-9)\n",
        "        return np.mean(log_likelihood)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Prediction\n",
        "    # -----------------------------\n",
        "    def predict(self, X):\n",
        "        activations, _ = self.forward(X)\n",
        "        return np.argmax(activations[-1], axis=1)\n",
        "\n",
        "# Create dummy dataset\n",
        "np.random.seed(0)\n",
        "X = np.random.randn(500, 2)\n",
        "y = (X[:, 0] * X[:, 1] > 0).astype(int)\n",
        "\n",
        "# Build model\n",
        "model = MLP([2, 16, 16, 2], activation=\"relu\")\n",
        "\n",
        "# Train\n",
        "model.fit(X, y, epochs=1000, lr=0.01, batch_size=32)\n",
        "\n",
        "# Predict\n",
        "preds = model.predict(X)\n",
        "accuracy = np.mean(preds == y)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N3imvoPI_sK",
        "outputId": "fe7a8348-7ce5-4ba3-9ef7-f802526ce94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6747\n",
            "Epoch 100, Loss: 0.1146\n",
            "Epoch 200, Loss: 0.0712\n",
            "Epoch 300, Loss: 0.0545\n",
            "Epoch 400, Loss: 0.0448\n",
            "Epoch 500, Loss: 0.0382\n",
            "Epoch 600, Loss: 0.0336\n",
            "Epoch 700, Loss: 0.0301\n",
            "Epoch 800, Loss: 0.0271\n",
            "Epoch 900, Loss: 0.0248\n",
            "Accuracy: 0.998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MultilayerPerceptron:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        # Initialize weights and biases with small random values\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def _sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Forward propagation: Input -> Hidden -> Output\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self._sigmoid(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self._sigmoid(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        # Calculate error at output layer\n",
        "        output_error = y - output\n",
        "        output_delta = output_error * self._sigmoid_derivative(output)\n",
        "\n",
        "        # Calculate error at hidden layer\n",
        "        hidden_error = output_delta.dot(self.W2.T)\n",
        "        hidden_delta = hidden_error * self._sigmoid_derivative(self.a1)\n",
        "\n",
        "        # Update weights and biases using Gradient Descent\n",
        "        self.W2 += self.a1.T.dot(output_delta) * self.lr\n",
        "        self.b2 += np.sum(output_delta, axis=0, keepdims=True) * self.lr\n",
        "        self.W1 += X.T.dot(hidden_delta) * self.lr\n",
        "        self.b1 += np.sum(hidden_delta, axis=0, keepdims=True) * self.lr\n",
        "\n",
        "    def train(self, X, y, epochs=1000):\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output)\n",
        "            if epoch % 100 == 0:\n",
        "                loss = np.mean(np.square(y - output))\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Return binary prediction based on 0.5 threshold\n",
        "        return (self.forward(X) > 0.5).astype(int)\n",
        "\n",
        "\n",
        "# XOR Data\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Create and train model\n",
        "mlp = MultilayerPerceptron(input_size=2, hidden_size=4, output_size=1)\n",
        "mlp.train(X, y, epochs=5000)\n",
        "\n",
        "# Test predictions\n",
        "print(\"Predictions:\", mlp.predict(X).flatten())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPL_sm9uJB77",
        "outputId": "c266c70a-efb1-4e88-bc18-48e89965a7aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2500\n",
            "Epoch 100, Loss: 0.2500\n",
            "Epoch 200, Loss: 0.2500\n",
            "Epoch 300, Loss: 0.2500\n",
            "Epoch 400, Loss: 0.2500\n",
            "Epoch 500, Loss: 0.2500\n",
            "Epoch 600, Loss: 0.2500\n",
            "Epoch 700, Loss: 0.2500\n",
            "Epoch 800, Loss: 0.2500\n",
            "Epoch 900, Loss: 0.2500\n",
            "Epoch 1000, Loss: 0.2500\n",
            "Epoch 1100, Loss: 0.2500\n",
            "Epoch 1200, Loss: 0.2500\n",
            "Epoch 1300, Loss: 0.2500\n",
            "Epoch 1400, Loss: 0.2500\n",
            "Epoch 1500, Loss: 0.2500\n",
            "Epoch 1600, Loss: 0.2500\n",
            "Epoch 1700, Loss: 0.2500\n",
            "Epoch 1800, Loss: 0.2500\n",
            "Epoch 1900, Loss: 0.2500\n",
            "Epoch 2000, Loss: 0.2500\n",
            "Epoch 2100, Loss: 0.2500\n",
            "Epoch 2200, Loss: 0.2500\n",
            "Epoch 2300, Loss: 0.2500\n",
            "Epoch 2400, Loss: 0.2500\n",
            "Epoch 2500, Loss: 0.2500\n",
            "Epoch 2600, Loss: 0.2500\n",
            "Epoch 2700, Loss: 0.2500\n",
            "Epoch 2800, Loss: 0.2500\n",
            "Epoch 2900, Loss: 0.2500\n",
            "Epoch 3000, Loss: 0.2500\n",
            "Epoch 3100, Loss: 0.2500\n",
            "Epoch 3200, Loss: 0.2500\n",
            "Epoch 3300, Loss: 0.2500\n",
            "Epoch 3400, Loss: 0.2500\n",
            "Epoch 3500, Loss: 0.2500\n",
            "Epoch 3600, Loss: 0.2500\n",
            "Epoch 3700, Loss: 0.2500\n",
            "Epoch 3800, Loss: 0.2500\n",
            "Epoch 3900, Loss: 0.2500\n",
            "Epoch 4000, Loss: 0.2500\n",
            "Epoch 4100, Loss: 0.2500\n",
            "Epoch 4200, Loss: 0.2500\n",
            "Epoch 4300, Loss: 0.2500\n",
            "Epoch 4400, Loss: 0.2500\n",
            "Epoch 4500, Loss: 0.2500\n",
            "Epoch 4600, Loss: 0.2500\n",
            "Epoch 4700, Loss: 0.2500\n",
            "Epoch 4800, Loss: 0.2500\n",
            "Epoch 4900, Loss: 0.2500\n",
            "Predictions: [1 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "class MLP():\n",
        "\n",
        "    \"\"\"\n",
        "    This is the MLP class used to feedforward and backpropagate the network across a defined number\n",
        "    of iterations and produce predictions. After iteration the predictions are assessed using\n",
        "    Binary Cross Entropy Cost function.\n",
        "    \"\"\"\n",
        "\n",
        "    print('Running...')\n",
        "\n",
        "    def __init__(self, design_matrix, Y, iterations=100000, lr=1e-1, input_layer = 2, hidden_layer = 3,output_layer =1):\n",
        "        self.design_matrix = design_matrix #design matrix attibute\n",
        "        self.iterations = iterations #iterations attibute\n",
        "        self.lr = lr #learning rate attibute\n",
        "        self.input_layer = input_layer #input layer attibute\n",
        "        self.hidden_layer = hidden_layer #hidden layer attibute\n",
        "        self.output_layer = output_layer #output layer attibute\n",
        "        self.weight_matrix_1 = np.random.randn(self.input_layer, self.hidden_layer) #weight attribute connecting to the hidden layer\n",
        "        self.weight_matrix_2 = np.random.randn(self.hidden_layer, self.output_layer)#weight attribute connecting to the output layer\n",
        "        self.cost = [] #cost list attribute\n",
        "        self.p_hats = [] #predictions list attribute\n",
        "\n",
        "    def sigmoid(self, x): # sigmoid function used at the hidden layer and output layer\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x): # sigmoid derivative used for backpropgation\n",
        "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
        "\n",
        "    def forward_propagation(self):#define function to feedforward the network\n",
        "        z = np.dot(self.design_matrix, self.weight_matrix_1) #linear transformation to the hidden layer\n",
        "        activation_func = self.sigmoid(z)#hidden layer activation function\n",
        "        zh = np.dot(activation_func, self.weight_matrix_2)#linear transformation to the output layer\n",
        "        p_hat = self.sigmoid(zh)#output layer prediction\n",
        "        return z, activation_func, zh, p_hat\n",
        "\n",
        "    def BCECost(self, y, p_hat): # binary cross entropy cost function\n",
        "        bce_cost = -(np.sum(y * np.log(p_hat) + (1 - y) * np.log(1 - p_hat))) / len(y)\n",
        "        return bce_cost\n",
        "\n",
        "    def backword_prop(self, z_1, activation_func, z_2, p_hat): #backpropagation\n",
        "        del_2_1 = p_hat - Y\n",
        "        partial_deriv_2 = np.dot(activation_func.T, del_2_1) #∂loss/∂p *∂p/∂zh * ∂zh/∂wh\n",
        "        del_1_1 = del_2_1\n",
        "        del_1_2 = np.multiply(del_1_1, self.weight_matrix_2.T)\n",
        "        del_1_3 = np.multiply(del_1_2, self.sigmoid_derivative(z_1))\n",
        "        partial_deriv_1 = np.dot(self.design_matrix.T, del_1_3) #∂loss/∂p * ∂p/∂zh * ∂zh/∂h * ∂h/∂z * ∂z/∂w\n",
        "        return partial_deriv_2, partial_deriv_1\n",
        "\n",
        "    def train(self):#train the network\n",
        "        for i in range(self.iterations): #loop based on number of iterations\n",
        "            z_1, activation_func, z_2, p_hat = self.forward_propagation()# feedforward\n",
        "            partial_deriv_2, partial_deriv_1 = self.backword_prop(z_1, activation_func, z_2, p_hat)#backpropgate\n",
        "            self.weight_matrix_1 = self.weight_matrix_1 - self.lr * partial_deriv_1#update weights connecting to the hidden layer (gradient descent)\n",
        "            self.weight_matrix_2 = self.weight_matrix_2 - self.lr * partial_deriv_2#update weights connecting to the output layer (gradient descent )\n",
        "            self.cost.append(self.BCECost(Y, p_hat))#store BCE cost in list\n",
        "            self.p_hats.append(p_hat)#store predictions in list\n",
        "\n",
        "\n",
        "        print('Training Complete')\n",
        "        print('----------------------------------------------------------------------------')\n",
        "\n",
        "# Prepare the XOR Logic Gate data: create an array for each training example x feature, and an array for each corrosponding y label.\n",
        "X = np.array([[1, 0], [0, 1], [0, 0], [1, 1]]) #input features (4 x 2 design matrix)\n",
        "Y = np.array([[1], [1], [0], [0]])#ground truth y labels (4x1)\n",
        "\n",
        "mlp = MLP(X,Y)#Pass data to the model (design matrix and y label)\n",
        "mlp.train() #Train the model\n",
        "\n",
        "#plot the cost function\n",
        "plt.grid()\n",
        "plt.plot(range(mlp.iterations),mlp.cost)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('BCE Cost Function')\n",
        "\n",
        "#Print predictions, number of iterations and the ground truth labels.\n",
        "print(f'\\n The MLP predictions for each training example, based on {mlp.iterations} iterations are:\\n\\n{np.round(mlp.p_hats[-1],2)}')\n",
        "print('\\n----------------------------------------------------------------------------')\n",
        "print(f'\\n The ground truth Y labels are are:\\n\\n{Y}')\n"
      ],
      "metadata": {
        "id": "PJl__Af7f0Hq",
        "outputId": "61764523-7193-43bb-de33-885ad4367cb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running...\n",
            "Training Complete\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            " The MLP predictions for each training example, based on 100000 iterations are:\n",
            "\n",
            "[[1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            " The ground truth Y labels are are:\n",
            "\n",
            "[[1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}