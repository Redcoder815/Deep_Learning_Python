{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_Python/blob/main/SingleLayerPerceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASS Perceptron:\n",
        "#     METHOD __init__ (input_size, learning_rate, epochs):\n",
        "#         Set self.weights = Array of zeros of length input_size\n",
        "#         Set self.bias = 0.0\n",
        "#         Set self.lr = learning_rate\n",
        "#         Set self.epochs = epochs\n",
        "#     METHOD activation_function (value):\n",
        "#         IF value >= 0 THEN RETURN 1\n",
        "#         ELSE RETURN 0\n",
        "#     METHOD forward_pass (input_vector):\n",
        "#         weighted_sum = (Dot Product of input_vector and self.weights) + self.bias\n",
        "#         RETURN result of activation_function(weighted_sum)\n",
        "#     METHOD train (training_inputs, labels):\n",
        "#         FOR each epoch from 1 to self.epochs:\n",
        "#             total_error = 0\n",
        "#             FOR each input_item and target_label in data:\n",
        "#                 1. Forward Pass:\n",
        "#                 prediction = forward_pass(input_item)\n",
        "\n",
        "#                 2. Error Calculation:\n",
        "#                 error = target_label - prediction\n",
        "\n",
        "#                 3. Backward Pass (Weight Update):\n",
        "#                 IF error is not 0:\n",
        "#                     update = self.lr * error\n",
        "#                     self.weights = self.weights + (update * input_item)\n",
        "#                     self.bias = self.bias + update\n",
        "#                     total_error = total_error + Absolute Value of error\n",
        "\n",
        "#             IF total_error == 0 THEN EXIT LOOP (Model Converged)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, input_size, learning_rate=0.01, epochs=100):\n",
        "        # Initialize weights and bias to small random numbers or zeros\n",
        "        self.weights = np.zeros(input_size)\n",
        "        self.bias = 0.0\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def activation_fn(self, x):\n",
        "        \"\"\"Heaviside step function: returns 1 if x >= 0, else 0.\"\"\"\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass: Calculate weighted sum + bias and apply activation.\"\"\"\n",
        "        linear_output = np.dot(x, self.weights) + self.bias\n",
        "        return self.activation_fn(linear_output)\n",
        "\n",
        "    def train(self, X, y):\n",
        "        \"\"\"Backward pass error calculation and weight updates.\"\"\"\n",
        "        for epoch in range(self.epochs):\n",
        "            total_error = 0\n",
        "            for xi, target in zip(X, y):\n",
        "                # 1. Forward Pass\n",
        "                prediction = self.forward(xi)\n",
        "\n",
        "                # 2. Error Calculation\n",
        "                error = target - prediction\n",
        "\n",
        "                # 3. Weight and Bias Update (Perceptron Rule)\n",
        "                if error != 0:\n",
        "                    update = self.lr * error\n",
        "                    self.weights += update * xi\n",
        "                    self.bias += update\n",
        "                    total_error += abs(error)\n",
        "\n",
        "            # Optional: Stop if the model has converged\n",
        "            if total_error == 0:\n",
        "                print(f\"Converged at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict labels for a batch of inputs.\"\"\"\n",
        "        return np.array([self.forward(xi) for xi in X])\n",
        "\n",
        "# Example Usage: Implementing an AND Gate\n",
        "if __name__ == \"__main__\":\n",
        "    # Training Data (AND Logic)\n",
        "    X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    y_train = np.array([0, 0, 0, 1])\n",
        "\n",
        "    # Initialize and Train\n",
        "    model = Perceptron(input_size=2, learning_rate=0.1, epochs=10)\n",
        "    model.train(X_train, y_train)\n",
        "\n",
        "    # Inference\n",
        "    print(f\"Final Weights: {model.weights}\")\n",
        "    print(f\"Final Bias: {model.bias}\")\n",
        "    print(f\"Predictions: {model.predict(X_train)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wBMRkhdwxKBd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}